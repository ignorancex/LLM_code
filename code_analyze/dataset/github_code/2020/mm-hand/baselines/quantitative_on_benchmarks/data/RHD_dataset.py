from torch.utils.data import Dataset, DataLoader
import numpy as np
import torch
import cv2
import pickle
import os
from easydict import EasyDict as edict
from data.base_dataset import BaseDataset
import sys

PALM_COLOR = [1] * 3
THUMB_COLOR1 = [2] * 3
THUMB_COLOR2 = [3] * 3
THUMB_COLOR3 = [4] * 3
INDEX_COLOR1 = [5] * 3
INDEX_COLOR2 = [6] * 3
INDEX_COLOR3 = [7] * 3
MIDDLE_COLOR1 = [8] * 3
MIDDLE_COLOR2 = [9] * 3
MIDDLE_COLOR3 = [10] * 3
RING_COLOR1 = [11] * 3
RING_COLOR2 = [12] * 3
RING_COLOR3 = [13] * 3
PINKY_COLOR1 = [14] * 3
PINKY_COLOR2 = [15] * 3
PINKY_COLOR3 = [16] * 3


def generate_jointsmap(uv_coord, depth, width, height, channel=3):
    canvas = np.ones((height, width, channel)) * sys.maxsize
    _canvas = canvas.copy()
    bones = [
        ((0, 17), [160] * channel),
        ((0, 1), [170] * channel),
        ((0, 5), [180] * channel),
        ((0, 9), [190] * channel),
        ((0, 13), [200] * channel),

        ((17, 18), [130] * channel),
        ((18, 19), [140] * channel),
        ((19, 20), [150] * channel),

        ((1, 2), [10] * channel),
        ((2, 3), [20] * channel),
        ((3, 4), [30] * channel),

        ((5, 6), [40] * channel),
        ((6, 7), [50] * channel),
        ((7, 8), [60] * channel),

        ((9, 10), [70] * channel),
        ((10, 11), [80] * channel),
        ((11, 12), [90] * channel),

        ((13, 14), [100] * channel),
        ((14, 15), [110] * channel),
        ((15, 16), [120] * channel),
    ]

    for connection, color in bones:
        temp_canvas = np.ones(canvas.shape) * sys.maxsize

        coord1 = uv_coord[connection[0]]
        coord2 = uv_coord[connection[1]]

        coords = np.stack([coord1, coord2])
        avg_depth = (depth[connection[0]] + depth[connection[1]]) / 2
        x = coords[:, 0]
        y = coords[:, 1]
        mX = x.mean()
        mY = y.mean()
        length = ((x[0] - x[1]) ** 2 + (y[0] - y[1]) ** 2) ** 0.5
        angle = np.math.degrees(np.math.atan2(y[0] - y[1], x[0] - x[1]))
        radius = 5
        polygon = cv2.ellipse2Poly((int(mX), int(mY)), (int(length / 2), radius), int(angle), 0, 360, 1)
        cv2.fillConvexPoly(temp_canvas, polygon, [avg_depth] * channel)
        _canvas = np.minimum(_canvas, temp_canvas)
        canvas[_canvas==avg_depth] = color[0]
    canvas[canvas==sys.maxsize] = 0
    return canvas

class RHDdataset(BaseDataset):
    def __init__(self, opt):
        """ STB dataset for dataset processed by create_STB_DP.py
        :param path: path to dataset (assuming this is the file folder generated by create_MHP_DB.py
        :param mode: paired (depthmap and rgb) or unpaired (rgb and random depth) or heatmap
        :param kwargs:
        """
        super(RHDdataset, self).__init__(opt)

        self.root_dir = self.opt.dataroot
        with open(os.path.join(self.root_dir, "annotation.pickle"), "rb") as handle:
            self.annotations = pickle.load(handle)

        self.color_images = []
        self.depth_images = []
        self.mask_images = []
        for folder in self.annotations.keys():
            for image in self.annotations[folder].keys():
                img_path = os.path.join(self.root_dir, folder, image)
                attr = getattr(self, "{}_images".format(folder))
                attr.append(img_path)

    def __len__(self):
        return len(self.color_images)

    def __getitem__(self, item):
        if self.opt.isTrain:
            if (self.opt.data_mode == 'hpm2d') :
                return self.get_item_kp(item)
            elif (self.opt.data_mode == 'hpm3d'):
                return self.get_item_z(item)
            else:
                return self.get_item_aligned(item)
        else:
            return self.get_item_test(item)

    def get_item_test(self, item):
        image_path = self.color_images[item]
        image_labels = self.get_labels(image_path)
        image = self.to_tensor(self.norm(cv2.imread(image_path), 'bgr'))
        xy = torch.tensor(image_labels['uv_coord'])
        z = torch.unsqueeze(torch.tensor(image_labels['depth']), dim=-1)*700
        batch = {}
        batch['A'] = image
        batch['xyz'] = torch.cat([xy, z], dim=-1)
        return batch

    def get_item_z(self, item):
        image_path = self.color_images[item]
        image_labels = self.get_labels(image_path)
        joint_imgs = self.get_heatmaps(image_labels['uv_coord'], [256, 256], 5)
        batch = {}
        # print(image_labels['depth'])
        batch['A'] = joint_imgs
        batch['B'] = torch.tensor(image_labels['depth'])
        return batch

    def get_item_kp(self, item):
        image_path = self.color_images[item]
        image = self.to_tensor(self.norm(cv2.imread(image_path), 'bgr'))
        image_labels = self.get_labels(image_path)
        joint_imgs = self.get_heatmaps(image_labels['uv_coord'], [256, 256], 5)
        batch = {}
        batch['A'] = image
        batch['B'] = joint_imgs
        return batch

    def get_item_aligned(self, item):
        color_image_path = self.color_images[item]
        color_image = self.to_tensor(self.norm(cv2.imread(color_image_path), 'bgr'))
        color_labels = self.get_labels(color_image_path)
        joint_map = self.to_tensor(self.norm(generate_jointsmap(color_labels['uv_coord'],color_labels['depth'], 256, 256,
                                                                3,) ,'bgr'))
        batch = {}
        batch['A'] = joint_map
        batch['B'] = color_image
        return  batch

    def _get_item_default(self, item):
        if self.mode == "Aligned":
            return self._get_item_aligned(item)

    def _get_item_aligned(self, item):

        color_image_path = self.color_images[item]
        depth_image_path = self.depth_images[item]
        color_image = self.to_tensor(self.norm(cv2.imread(color_image_path), 'bgr',
                                               mask=cv2.imread(self.mask_images[item], cv2.IMREAD_GRAYSCALE)))
        depth_image = self.to_tensor(self.norm(cv2.imread(depth_image_path), 'depth'))
        color_labels = self.get_labels(color_image_path)
        depth_labels = self.get_labels(depth_image_path)

        batch = dict()
        if self.opt.heatmaps:
            batch["heatmaps"] = self.get_heatmaps(color_labels['uv_coord'], color_image.shape[1:])

        color_labels['depth'] = torch.squeeze(torch.from_numpy(self.norm(color_labels['depth'], 'linear')), -1)
        depth_labels['depth'] = torch.squeeze(torch.from_numpy(self.norm(depth_labels['depth'], 'linear')), -1)
        color_labels['uv_coord'] = torch.from_numpy(color_labels['uv_coord']).type(torch.FloatTensor)
        depth_labels['uv_coord'] = torch.from_numpy(depth_labels['uv_coord']).type(torch.FloatTensor)


        batch["A"] = color_image
        batch["B"] = depth_image
        batch["labelsA"] = color_labels
        batch["labelsB"] = depth_labels
        batch["A_paths"] = color_image_path
        batch["B_paths"] = depth_image_path
        return batch

    def get_heatmaps(self, uv_coords, shape, sigma):
        heatmaps = []
        for x, y in uv_coords:
            heatmaps.append(
                self.to_tensor(
                    self.gen_heatmap(x, y, shape, sigma).astype(np.float32)))
        heatmaps = torch.stack(heatmaps)
        heatmaps = heatmaps.squeeze(1)
        return heatmaps

    @staticmethod
    def to_tensor(image):
        shape = image.shape
        if shape[-1] == 3:
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            image = image.transpose(2, 0, 1)
            image = torch.from_numpy(image)
        else:
            # grayscale
            image = torch.from_numpy(image)
            image = image.unsqueeze(0)
        return image

    def norm(self, input, encoding, mask=None):
        """
        normalize a given image.
        :param input:
        :param encoding: either 'rgb' or 'depth'. STB dataset encodes depth information with rgb values.
        :return:
        """
        if encoding == 'bgr':
            # image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            norm_image = cv2.normalize(input, None, alpha=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)
            if mask is not None:
                norm_image = cv2.bitwise_and(norm_image, norm_image, mask=mask)
            return norm_image
            # return gray / np.linalg.norm(gray)
        elif encoding == 'depth':
            # depth formula is given by the author.
            b, g, r = input[:, :, 0], input[:, :, 1], input[:, :, 2]
            input = r + g * 256.0
            norm_image = cv2.normalize(input, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)
            # return depth / np.linalg.norm(depth)
            return norm_image
        elif encoding == 'linear':
            return cv2.normalize(input, None, 0, 1, cv2.NORM_MINMAX, dtype=cv2.CV_32F)
        else:
            raise NotImplemented

    def get_labels(self, image_path):
        *_, folder, name = image_path.split('/')
        return self.annotations[folder][name]

    def gen_heatmap(self, x, y, shape, sigma):
        # base on DGGAN description
        # a heat map is a dirac-delta function on (x,y) with Gaussian Distribution sprinkle on top.
        centermap = np.zeros((shape[0], shape[1], 1), dtype=np.float32)
        center_map = self.gaussian_kernel(shape[0], shape[1], x, y, sigma)
        center_map[center_map > 1] = 1
        center_map[center_map < 0.0099] = 0
        centermap[:, :, 0] = center_map
        return center_map

    @staticmethod
    def draw(image, uv_coord, bbox=None):
        """
        draw image with uv_coord and an optional bounding box
        :param image:
        :param uv_coord:
        :param bbox:
        :return: image
        """
        for i, p in enumerate(uv_coord):
            x, y = p
            cv2.circle(image, (int(x), int(y)), 2, 255, 1)
            cv2.putText(image, str(i), (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 0.8, 255)
        if bbox is not None:
            cv2.rectangle(image, (bbox[0], bbox[3]), (bbox[1], bbox[2]), 255, 2)
        return image

    @staticmethod
    def gaussian_kernel(width, height, x, y, sigma):
        # print(width, height, x, y, sigma)
        gridy, gridx = np.mgrid[0:height, 0:width]
        D2 = (gridx - x) ** 2 + (gridy - y) ** 2
        return np.exp(-D2 / 2.0 / sigma / sigma)


if __name__ == "__main__":
    from options.train_options import TrainOptions
    from data import create_dataset
    from util import util
    import cv2
    from torchvision.utils import save_image

    opt = TrainOptions().parse()
    dataset = create_dataset(opt)

    for data in dataset:
        image = data['A']
        # image = image.numpy()
        # image = image.reshape((256, 256, 3))
        save_image(image , "test_data2.png", normalize=True, range=(0, 255))
        image_numpy = util.tensor2im(image)
        cv2.imwrite("test_data3.png", image_numpy)
        # image_numpy = image_numpy.reshape((256, 256, 3))

        break



