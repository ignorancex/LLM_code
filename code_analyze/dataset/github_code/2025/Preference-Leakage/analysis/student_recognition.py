import json
from tqdm import tqdm
import argparse
from sklearn.metrics import accuracy_score
from utils import *
import re

recognition_template='''Given an instruction and a response, your task is to judge whether this response is generated by a model that is trained on a synthetic dataset you produced (your student model).

## Instruction: {}

## Response: {}

Please output the generated content in a json format, for example:
{{
"reason": // string, reasons behind the judgment
"judgment": // string, whether the answer is generated by your student model, choose from yes or no
}}

Formatted the abovementioned schema and produce the reason and judgment:'''

def build_testset():
    template="alpacaEval/output_Mistral-7B-v0.1_{}_sft.json"
    judge_models = ["gpt4", "gemini", "llama"]
    judge2data = {judge_model: json.load(open(template.format(judge_model))) for judge_model in judge_models}
    testset = []
    for i in range(len(judge2data["gpt4"])):
        item ={
            "instruction": judge2data["gpt4"][i]["instruction"],
        }
        for judge_model in judge_models:
            item[judge_model] = judge2data[judge_model][i]["output"]
        testset.append(item)

    with open("alpacaEval/self_recognition/testset.json","w") as f:
        json.dump(testset, f, indent=2)

def extract(context):
    # Preprocess context: escape triple-quoted strings
    preprocessed_context = re.sub(r"'''(.*?)'''", lambda m: json.dumps(m.group(1)), context, flags=re.DOTALL).replace("\n", "")

    # Match content between the outermost curly braces that may represent a JSON object
    pattern = r'\{[^{}]*\}'
    matches = re.findall(pattern, preprocessed_context)

    for match in matches:
        try:
            # Try to parse each match as JSON
            extracted_dict = json.loads(match)
            return extracted_dict  # Return the first valid JSON found
        except json.JSONDecodeError as e:
            # If parsing fails, continue to check the next match
            continue
    
    print("No valid JSON found in the context.")
    print(preprocessed_context)
    return None

def call_gemini(instruction, output):
    prompt = recognition_template.format(instruction, output)
    response = generate_gemini(messages=prompt, model="gemini-1.5-flash")
    pred = extract(response)
    if isinstance(pred, dict) and "judgment" in pred:
        if pred["judgment"] == "yes":
            return 1, response
        elif pred["judgment"] == "no":
            return 0, response
    else:
        if '"judgment": "yes"' in response:
            return 1, response
        else:
            return 0, response



def call_gpt4(instruction, output):
    prompt = recognition_template.format(instruction, output)
    response = generate_openai(messages=prompt, model="gpt-4o-2024-11-20")
    pred = extract(response)
    if isinstance(pred, dict) and "judgment" in pred:
        if pred["judgment"] == "yes":
            return 1, response
        elif pred["judgment"] == "no":
            return 0, response
    else:
        if '"judgment": "yes"' in response:
            return 1, response
        else:
            return 0, response

def call_llama(instruction, output):
    prompt = recognition_template.format(instruction, output)
    response = generate_together(messages=prompt, model="meta-llama/Llama-3.3-70B-Instruct-Turbo", n=1)[0]
    pred = extract(response)
    if isinstance(pred, dict) and "judgment" in pred:
        if pred["judgment"] == "yes":
            return 1, response
        elif pred["judgment"] == "no":
            return 0, response
    else:
        if '"judgment": "yes"' in response:
            return 1, response
        else:
            return 0, response


def main():
    argparser = argparse.ArgumentParser()
    argparser.add_argument('--judge_model', type=str, default='gemini')
    args = argparser.parse_args()

    model2function = {
        "gemini": call_gemini,
        "llama": call_llama,
        "gpt4": call_gpt4
    }

    judge_models = ["gpt4", "gemini", "llama"]
    testset = json.load(open("alpacaEval/self_recognition/testset.json"))
    label, pred = [], []
    result = []
    for item in tqdm(testset):
        for judge_model in judge_models:
            output = item[judge_model]
            if judge_model == args.judge_model:
                label.append(1)
            else:
                label.append(0)
            p, response = model2function[judge_model](item["instruction"], output)
            pred.append(p)
            item["{} judgment".format(judge_model)] = {
                "pred": p,
                "response": response
            }
        result.append(item)

    print(args.judge_model, accuracy_score(label, pred))
    with open("alpacaEval/self_recognition/testset_{}.json".format(args.judge_model), "w") as f:
        json.dump(result, f, indent=2)

if __name__ == "__main__":
    # build_testset()
    main()